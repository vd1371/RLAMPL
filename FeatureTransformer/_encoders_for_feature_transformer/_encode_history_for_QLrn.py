import numpy as np

from ._evaluate_Q_for_Q_learning import _evaluate_Q_for_Q_learning
from ._get_SARS_for_step import get_SARS_for_step

def encode_history_for_QLrn(LrnObjs,
							R_hist,
							P_hist,
							models_holder,
							target_models_holder):
		'''Encoding the rewards to target values
		A_hist {10: [[3, 3, 3, 3, 3, 3, 1, 3, 1, 2],
						[1, 2, 2, 0, 2, 2, 2, 0, 2, 2],
						[1, 1, 2, 2, 2, 0, 1, 2, 3, 2]]}

		R_hist {10: [[11.5, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0],
						[7.12, -10.0, -10.0,   0.0, -10.0, -10.0, -10.0,   0.0, -10.0, -10.0],
						[7.1s, -10.0, -10.0, -10.0, -10.0,   0.0, -10.0, -10.0, -10.0, -10.0]]}

		nextS_hist {10: [[[-0.5, -0.1, -0.1, -1.0, -1.0, -1.0, 0.6, -0.9, -0.0],
						[-0.5, -0.05, -0.05, -1.0, -1.0, -1.0, 0.05, -0.8, -0.002],
						[-0.5, 0.0, 0.0, -1.0, -1.0, -1.0, 0.0, -0.7, -0.015],
						[-0.5, 0.05, 0.05, -1.0, -1.0, -1.0, 0.0, -0.6, -0.014], 
						[-0.5, 0.1, 0.1, -1.0, -1.0, -1.0, 0.0, -0.5, 0.005],
						[-0.5, 0.15, 0.15, -1.0, -1.0, -1.0, 0.0, -0.4, -5.275011391736584e-05],
						[-0.45, 0.2, 0.2, -1.0, -1.0, -1.0, 0.0, -0.3, -0.008039984097389707],
						[-0.5, 0.25, 0.25, -1.0, -1.0, -1.0, 0.0, -0.2, -0.008869700750765874],
						[-0.45, 0.3, -0.5, -1.0, -1.0, -1.0, 0.0, -0.1, 0.013458107564365496],
						[-0.4, 0.35, -0.45, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0008054610102850024]],

					[[-0.5, -0.1, -0.1, -1.0, -1.0, -1.0, 0.64974750144, -0.9, -0.008730423131534382],
						[-0.5, -0.05, -0.05, -1.0, -1.0, -1.0, 0.05486230099180159, -0.8, -0.002419572145080373],
						[-0.5, 0.0, 0.0, -1.0, -1.0, -1.0, 0.0, -0.7, -0.015426354380747354],
						[-0.5, 0.05, 0.05, -1.0, -1.0, -1.0, 0.0, -0.6, -0.011006740914700774],
						[-0.5, 0.1, 0.1, -1.0, -1.0, -1.0, 0.0, -0.5, 0.005775424631357495],
						[-0.5, 0.15, 0.15, -1.0, -1.0, -1.0, 0.0, -0.4, -5.275011391736584e-05],
						[-0.45, 0.2, 0.2, -1.0, -1.0, -1.0, 0.0, -0.3, -0.008039984097389707],
						[-0.5, 0.25, 0.25, -1.0, -1.0, -1.0, 0.0, -0.2, -0.008869700750765874],
						[-0.45, 0.3, -0.5, -1.0, -1.0, -1.0, 0.0, -0.1, 0.013458107564365496],
						[-0.4, 0.35, -0.45, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0008054610102850024]],

					[[-0.5, -0.1, -0.1, -1.0, -1.0, -1.0, 0.64974750144, -0.9, -0.008730423131534382],
						[-0.5, -0.05, -0.05, -1.0, -1.0, -1.0, 0.05486230099180159, -0.8, -0.002419572145080373],
						[-0.5, 0.0, 0.0, -1.0, -1.0, -1.0, 0.0, -0.7, -0.015426354380747354],
						[-0.5, 0.05, 0.05, -1.0, -1.0, -1.0, 0.0, -0.6, -0.011006740914700774],
						[-0.5, 0.1, 0.1, -1.0, -1.0, -1.0, 0.0, -0.5, 0.005775424631357495],
						[-0.5, 0.15, 0.15, -1.0, -1.0, -1.0, 0.0, -0.4, -5.275011391736584e-05],
						[-0.45, 0.2, 0.2, -1.0, -1.0, -1.0, 0.0, -0.3, -0.008039984097389707],
						[-0.5, 0.25, 0.25, -1.0, -1.0, -1.0, 0.0, -0.2, -0.008869700750765874],
						[-0.45, 0.3, -0.5, -1.0, -1.0, -1.0, 0.0, -0.1, 0.013458107564365496],
						[-0.4, 0.35, -0.45, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0008054610102850024]]]}
		'''

		# Iterating over asset IDs

		S_hist, A_hist, _, nextS_hist, _, _, _ = LrnObjs.episode_holder.get()
		
		
		targets = {}
		for id_ in nextS_hist:

			targets[id_] = [[] for _ in range(LrnObjs.n_elements)]

			for step in range(LrnObjs.settings.n_steps):

				S_at_step, R_at_step, P_at_step, A_at_step, nextS_at_step = \
					get_SARS_for_step(S_hist, R_hist, P_hist,
											A_hist, nextS_hist,
											id_, step, LrnObjs.n_elements)



				if step == LrnObjs.settings.n_steps - 1:
					discounted_r = R_at_step + P_at_step
					target = np.zeros((LrnObjs.n_elements, LrnObjs.dim_actions))

				else:

					target, Q = _evaluate_Q_for_Q_learning(LrnObjs.is_double,
														id_,
														nextS_at_step,
														models_holder,
														target_models_holder)

					discounted_r = R_at_step + P_at_step + LrnObjs.GAMMA * Q

				target[np.arange(LrnObjs.n_elements), A_at_step] = discounted_r

				for ne in range(LrnObjs.n_elements):
					targets[id_][ne].append(target[ne])

		return targets